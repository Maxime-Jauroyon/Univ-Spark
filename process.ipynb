{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob \n",
    "# !pip install pyspark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"localhost\"\n",
    "STREAM_PORT = 9999\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sc.setCheckpointDir(\"spark_checkpoint\")\n",
    "ssc = StreamingContext(sc, 10)\n",
    "submissions = ssc.socketTextStream(HOST, STREAM_PORT)\n",
    "sc.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_submission(message):\n",
    "\n",
    "    submission = json.loads(message)\n",
    "\n",
    "    title = submission['message']\n",
    "    metadata = submission['metadata']\n",
    "    author = metadata['author_name']\n",
    "    date = metadata['date']\n",
    "    score = metadata['score']\n",
    "    num_comments = metadata['num_comments']\n",
    "    upvote_ratio = metadata['upvote_ratio']\n",
    "    text = metadata['text']\n",
    "    subreddit_name = metadata['subreddit_name']\n",
    "\n",
    "    title_polarity, title_subjectivity = TextBlob(title).sentiment\n",
    "    text_polarity, text_subjectivity = TextBlob(text).sentiment\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'text': text,\n",
    "        'author': author,\n",
    "        'date': date,\n",
    "        'score': score,\n",
    "        'num_comments': num_comments,\n",
    "        'upvote_ratio': upvote_ratio,\n",
    "        'text': text,\n",
    "        'subreddit_name': subreddit_name,\n",
    "        'subreddit_hash': hash(subreddit_name),\n",
    "        'title_polarity': title_polarity,\n",
    "        'title_subjectivity': title_subjectivity,\n",
    "        'text_polarity': text_polarity,\n",
    "        'text_subjectivity': text_subjectivity\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add processing to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = submissions.map(process_submission)\n",
    "# submissions.pprint()\n",
    "training_data = submissions.map(lambda x: Vectors.dense(\n",
    "    [x['title_polarity'], x['title_subjectivity'], x['text_polarity'], x['text_subjectivity'], x[\"subreddit_hash\"]]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "model = StreamingKMeans(k, decayFactor=1.0).setRandomCenters(5, 1.0, 0)\n",
    "model.trainOn(training_data)\n",
    "result = model.predictOn(training_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = result.map(lambda cluster: (cluster, 1))\n",
    "# window of size 30s, and slides by 10s (very arbitrary)\n",
    "cluseter_counts = pairs.reduceByKeyAndWindow(\n",
    "    lambda x, y: x + y, lambda x, y: x - y, 30, 10)\n",
    "cluseter_counts.pprint()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create viz.json if doesn't exist and set every cluster size to 0\n",
    "with open(\"viz.json\", \"w\") as f:\n",
    "    total_data = {}\n",
    "    for i in range(k):\n",
    "        total_data[str(i)] = 0\n",
    "    json.dump(total_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cluster_sizes(rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        for x in rdd.collect():\n",
    "            with open(\"viz.json\", \"r\") as f:\n",
    "                total_data = json.load(f)\n",
    "            total_data[str(x[0])] += x[1]\n",
    "            with open(\"viz.json\", \"w\") as f:\n",
    "                json.dump(total_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the cluster sizes in sliding windows\n",
    "cluseter_counts.foreachRDD(lambda rdd: update_cluster_sizes(rdd))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
