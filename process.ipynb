{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob \n",
    "# !pip install pyspark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"localhost\"        \n",
    "STREAM_PORT = 9999     \n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setCheckpointDir(\"spark_checkpoint\")   \n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "submissions = ssc.socketTextStream(HOST, STREAM_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_submission(submission):\n",
    "\n",
    "    submission = json.loads(submission)\n",
    "\n",
    "    title = submission['title']\n",
    "    metadata = submission['metadata']\n",
    "    author = metadata['author']\n",
    "    date = metadata['date']\n",
    "    score = metadata['score']\n",
    "    num_comments = metadata['num_comments']\n",
    "    upvote_ratio = metadata['upvote_ratio']\n",
    "    text = submission['text']\n",
    "    subreddit_name = submission['subreddit_name']\n",
    "\n",
    "    title_polarity, title_subjectivity = TextBlob(title).sentiment\n",
    "    text_polarity, text_subjectivity = TextBlob(text).sentiment\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'text': text,\n",
    "        'author': author,\n",
    "        'date': date,\n",
    "        'score': score,\n",
    "        'num_comments': num_comments,\n",
    "        'upvote_ratio': upvote_ratio,\n",
    "        'text': text,\n",
    "        'subreddit_name': subreddit_name,\n",
    "        'title_polarity': title_polarity,\n",
    "        'title_subjectivity': title_subjectivity,\n",
    "        'text_polarity': text_polarity,\n",
    "        'text_subjectivity': text_subjectivity\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add processing to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = submissions.map(process_submission)\n",
    "training_data = submissions.map(lambda x: Vectors.dense(\n",
    "    [x['title_polarity'], x['title_subjectivity'], x['text_polarity'], x['text_subjectivity']]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StreamingKMeans(k=4, decayFactor=1.0).setRandomCenters(4, 1.0, 0)\n",
    "model.trainOn(training_data)\n",
    "result = model.predictOn(training_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = result.map(lambda cluster: (f'cluster-{cluster+1}', 1))  \n",
    "# window of size 30s, and slides by 10s (very arbitrary)\n",
    "cluseter_counts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)\n",
    "# print the results\n",
    "print(f'Cluster counts: {cluseter_counts}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a label encoder object\n",
    "# le = LabelEncoder()\n",
    "# # fit the encoder to the pandas column\n",
    "# le.fit(submissions.map(lambda x: x['subreddit_name']).collect())\n",
    "# # apply the fitted encoder to the pandas column\n",
    "# submissions = submissions.map(lambda x: (le.transform([x['subreddit_name']])[0], x['title_polarity'], x['title_subjectivity'], x['text_polarity'], x['text_subjectivity']))\n",
    "# training_data = submissions.map(lambda x: Vectors.dense([x[1], x[2], x[3], x[4]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_sentiment_label(text):\n",
    "#     blob = TextBlob(text)\n",
    "#     if blob.sentiment.polarity > 0:\n",
    "#         return \"positive\"\n",
    "#     elif blob.sentiment.polarity < 0:\n",
    "#         return \"negative\"\n",
    "#     else:\n",
    "#         return \"neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RedditAPIClient:\n",
    "#     def __init__(self, host, port):\n",
    "#         self.host = host\n",
    "#         self.port = port\n",
    "#         self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "#     def connect(self):\n",
    "#         self.sock.connect((self.host, self.port))\n",
    "\n",
    "#     def disconnect(self):\n",
    "#         self.sock.close()\n",
    "\n",
    "#     def process_data(self, data):\n",
    "#         # Parse the JSON data received from the server\n",
    "#         message = json.loads(data)['message']\n",
    "#         metadata = json.loads(data)['metadata']\n",
    "\n",
    "#         # Perform sentiment analysis on the message using TextBlob\n",
    "#         blob = TextBlob(message)\n",
    "#         sentiment = blob.sentiment.polarity\n",
    "\n",
    "#         # Return the metadata and sentiment score as a tuple\n",
    "#         return (metadata, sentiment)\n",
    "\n",
    "#     def run_spark(self):\n",
    "#         # Configure Spark\n",
    "#         conf = SparkConf().setAppName(\"Reddit Sentiment Analysis\")\n",
    "#         sc = SparkContext(conf=conf)\n",
    "\n",
    "#         # Create a DStream from the socket\n",
    "#         dstream = sc.socketTextStream(self.host, self.port)\n",
    "\n",
    "#         # Process the data stream using Spark and TextBlob\n",
    "#         results = dstream.map(self.process_data).filter(lambda x: x[1] != 0)\n",
    "\n",
    "#         # Print the results\n",
    "#         results.pprint()\n",
    "\n",
    "#         # Start the streaming context\n",
    "#         sc.start()\n",
    "#         sc.awaitTermination()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Initialize the client\n",
    "#     client = RedditAPIClient('localhost', 12345)\n",
    "\n",
    "#     # Connect to the server\n",
    "#     client.connect()\n",
    "\n",
    "#     # Run Spark on the data stream\n",
    "#     client.run_spark()\n",
    "\n",
    "#     # Disconnect from the server\n",
    "#     client.disconnect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
